{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","#!pip install transformers\n","from transformers import BertTokenizer, BertModel\n","import os\n","import time\n","import numpy as np\n","import pickle as pkl\n","import pandas as pd\n","import json\n","import nltk\n","from nltk import sent_tokenize\n","from tqdm import tqdm\n","nltk.download('punkt')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["ertsc imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from keras_preprocessing.sequence import pad_sequences\n","from transformers import BertForSequenceClassification, BertTokenizer, BertConfig"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["r model imports"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torch import nn\n","    \n","raw_input_path = \"./Input_Data/\"\n","output_path = \"./Output_Data/\"\n","emb_output_path = \"./Output_Data/Emb_Output_Data/\"\n","rr_output_path = \"./Output_Data/RR_Output_Data/\"\n","file_rr_output_path = \"./Output_Data/File_RR/\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["os.makedirs(emb_output_path, exist_ok = True)\n","print(\"Created emb_output_path ---->\",emb_output_path)\n","os.makedirs(rr_output_path, exist_ok = True)\n","print(\"Created rr_output_path ---->\",rr_output_path)\n","os.makedirs(file_rr_output_path, exist_ok = True)\n","print(\"Created file_rr_output_path ---->\",file_rr_output_path)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["oad BERTSC Model for bertsc embeddings"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["bertsc_model_path = \"./SiameseBERT_7labels_full/\"\n","print(\"Loading bertsc model from path::::: \",bertsc_model_path)\n","model_bertsc = BertForSequenceClassification.from_pretrained(bertsc_model_path, output_hidden_states=True)\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_bertsc.to(device)\n","tokenizer_bertsc = BertTokenizer.from_pretrained(bertsc_model_path)\n","print(\"::::::::::::: BertSC model loading success ::::::::::\")\n","#exit()\n","'''\n","    Data Conversion for BERTSC Model\n","'''\n","def json_to_df(data): \n","  sentences_1 = []\n","  sentences_2 = []\n","  for doc in data.keys():\n","    length_sentences = len(data[doc])\n","    for i,sentence in enumerate(data[doc]):\n","      if(i== length_sentences-1):\n","        break\n","      sentences_1.append(data[doc][i])\n","      sentences_2.append(data[doc][i+1])\n","  df = pd.DataFrame(list(zip(sentences_1, sentences_2)), columns =['Sentence 1', 'Sentence 2'])\n","  return df"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    Function to get imput ids for each sentences using the tokenizer<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def input_id_maker(dataf, tokenizer):\n","  input_ids = []\n","  lengths = []\n","  token_type_ids = []\n","  for i in tqdm(range(len(dataf['Sentence 1']))):\n","    sen1 = dataf['Sentence 1'].iloc[i]\n","    sen1_t = tokenizer.tokenize(sen1)\n","    sen2 = dataf['Sentence 2'].iloc[i]\n","    sen2_t = tokenizer.tokenize(sen2)\n","    if(len(sen1_t) > 253):\n","      sen1_t = sen1_t[:253]\n","    if(len(sen2_t) > 253):\n","      sen2_t = sen2_t[:253]\n","    CLS = tokenizer.cls_token\n","    SEP = tokenizer.sep_token\n","    sen_full = [CLS] + sen1_t + [SEP] + sen2_t + [SEP]\n","    tok_type_ids_0 = [0 for i in range(len(sen1_t)+2)]\n","    tok_type_ids_1 = [1 for i in range(512-len(sen1_t)-2)]\n","    tok_type_ids = tok_type_ids_0 + tok_type_ids_1\n","    token_type_ids.append(tok_type_ids)\n","    encoded_sent = tokenizer.convert_tokens_to_ids(sen_full)\n","    input_ids.append(encoded_sent)\n","    lengths.append(len(encoded_sent))\n","  input_ids = pad_sequences(input_ids, maxlen=256, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n","  tok_type_ids = []\n","  return input_ids, lengths, token_type_ids"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    This functions returns the attention mask for given input id<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def att_masking(input_ids):\n","  attention_masks = []\n","  for sent in input_ids:\n","    att_mask = [int(token_id > 0) for token_id in sent]\n","    attention_masks.append(att_mask)\n","  return attention_masks"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    This function returns the [CLS] embedding for a given input_id and attention mask<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_output_for_one_vec(input_id, att_mask):\n","  input_ids = torch.tensor(input_id)\n","  att_masks = torch.tensor(att_mask)\n","  input_ids = input_ids.unsqueeze(0)\n","  att_masks = att_masks.unsqueeze(0)\n","  model_bertsc.eval()\n","  input_ids = input_ids.to(device)\n","  att_masks = att_masks.to(device)\n","  with torch.no_grad():\n","      output = model_bertsc(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n","  vec = output[\"hidden_states\"][12][0][0]\n","  vec = vec.detach().cpu().numpy()\n","  return vec"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    Function to Concatenate Embeddings<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def get_s_emb(path_gen,path_bert):\n","    with open(path_gen, 'rb') as f:\n","        gen_emb = json.load(f)\n","    #load npy\n","    # print(\"Loading....\",path_bert)\n","    bert_emb = np.load(path_bert)\n","    gen_emb_np = {}\n","    for key,val in gen_emb.items():\n","        val_np = []\n","        for v in val:\n","            val_np.append(np.float32(v))\n","        gen_emb_np[key] = np.array(val_np)\n","    gen_emb = gen_emb_np\n","    s_emb = list()\n","    for i,key in enumerate(list(gen_emb.keys())):\n","        gei = gen_emb[key]\n","        if len(bert_emb)<1:\n","            bei_1 = [0] *768\n","            bei = [0]*768\n","        else:\n","            if i==0 :\n","                #print(i)\n","                bei_1 = [0] *768\n","                bei = bert_emb[0]\n","            elif i == (len(gen_emb)-1):\n","                #print(i)\n","                bei_1 = bert_emb[len(gen_emb)-2]\n","                bei = [0]*768\n","            else:\n","                bei_1 = bert_emb[i-1]\n","                bei = bert_emb[i]\n","        si = np.concatenate((bei_1,gei,bei),axis=0)\n","        s_emb.append(si)\n","    s_emb = np.array(s_emb)\n","    return s_emb"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["R model for RR results"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["SEED = 42\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed(SEED)\n","'''\n","Shift Module:\n","    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. \n","    The feature vectors are actually context-aware sentence embeddings. \n","    These are then fed to a feed-forward network to obtain emission scores for each class at each sentence.\n","'''\n","class LSTM_Emitter_Binary(nn.Module):\n","    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n","        self.dropout = nn.Dropout(drop)\n","        self.hidden2tag = nn.Linear(hidden_dim, n_tags)\n","        self.hidden = None\n","        self.device = device\n","    def init_hidden(self, batch_size):\n","        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n","    def forward(self, sequences):\n","        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n","        # initialize hidden state\n","        self.hidden = self.init_hidden(sequences.shape[0])\n","        # generate context-aware sentence embeddings (feature vectors)\n","        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n","        x, self.hidden = self.lstm(sequences, self.hidden)\n","        x_new = self.dropout(x)\n","        # generate emission scores for each class at each sentence\n","        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n","        x_new = self.hidden2tag(x_new)\n","        return x_new, x"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","RR Module:<br>\n","    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. <br>\n","    The feature vectors are actually context-aware sentence embeddings. <br>\n","    These are then fed to a feed-forward network to obtain emission scores for each class at each sentence.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LSTM_Emitter(nn.Module):\n","    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n","        super().__init__()\n","        self.hidden_dim = hidden_dim\n","        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n","        self.dropout = nn.Dropout(drop)\n","        self.hidden2tag = nn.Linear(2*hidden_dim, n_tags)\n","        self.hidden = None\n","        self.device = device\n","    def init_hidden(self, batch_size):\n","        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n","    def forward(self, sequences, hidden_binary):\n","        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n","        # initialize hidden state\n","        self.hidden = self.init_hidden(sequences.shape[0])\n","        # generate context-aware sentence embeddings (feature vectors)\n","        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n","        x, self.hidden = self.lstm(sequences, self.hidden)\n","        final = torch.zeros((x.shape[0], x.shape[1], 2*x.shape[2])).to(self.device)\n","        ## Concat the hidden states of both Shift and RR Module LSTM's and then pass through a linear layer to get emission scores for RR Module\n","        for batch_name, doc in enumerate(x):\n","            for i, sent in enumerate(doc):\n","                final[batch_name][i] = torch.cat((x[batch_name][i], hidden_binary[batch_name][i]),0)\n","        final = self.dropout(final)\n","        # generate emission scores for each class at each sentence\n","        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n","        final = self.hidden2tag(final)\n","        return final"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    A linear-chain CRF is fed with the emission scores at each sentence, <br>\n","    and it finds out the optimal sequence of tags by learning the transition scores.<br>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CRF(nn.Module):    \n","    def __init__(self, n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx = None):\n","        super().__init__()\n","        self.n_tags = n_tags\n","        self.SOS_TAG_IDX = sos_tag_idx\n","        self.EOS_TAG_IDX = eos_tag_idx\n","        self.PAD_TAG_IDX = pad_tag_idx\n","        self.transitions = nn.Parameter(torch.empty(self.n_tags, self.n_tags))\n","        self.init_weights()\n","    def init_weights(self):\n","        # initialize transitions from random uniform distribution between -0.1 and 0.1\n","        nn.init.uniform_(self.transitions, -0.1, 0.1)\n","        # enforce constraints (rows = from, cols = to) with a big negative number.\n","        # exp(-1000000) ~ 0\n","        # no transitions to SOS\n","        self.transitions.data[:, self.SOS_TAG_IDX] = -1000000.0\n","        # no transition from EOS\n","        self.transitions.data[self.EOS_TAG_IDX, :] = -1000000.0\n","        if self.PAD_TAG_IDX is not None:\n","            # no transitions from pad except to pad\n","            self.transitions.data[self.PAD_TAG_IDX, :] = -1000000.0\n","            self.transitions.data[:, self.PAD_TAG_IDX] = -1000000.0\n","            # transitions allowed from end and pad to pad\n","            self.transitions.data[self.PAD_TAG_IDX, self.EOS_TAG_IDX] = 0.0\n","            self.transitions.data[self.PAD_TAG_IDX, self.PAD_TAG_IDX] = 0.0\n","    def forward(self, emissions, tags, mask = None):\n","        ## emissions: tensor[batch_size, seq_len, n_tags]\n","        ## tags: tensor[batch_size, seq_len]\n","        ## mask: tensor[batch_size, seq_len], indicates valid positions (0 for pad)\n","        return -self.log_likelihood(emissions, tags, mask = mask)\n","    \n","    def log_likelihood(self, emissions, tags, mask = None):                   \n","        if mask is None:\n","            mask = torch.ones(emissions.shape[:2])\n","        scores = self._compute_scores(emissions, tags, mask = mask)\n","        partition = self._compute_log_partition(emissions, mask = mask)\n","        return torch.sum(scores - partition)\n","    \n","    # find out the optimal tag sequence using Viterbi Decoding Algorithm\n","    def decode(self, emissions, mask = None):      \n","        if mask is None:\n","            mask = torch.ones(emissions.shape[:2])\n","        scores, sequences = self._viterbi_decode(emissions, mask)\n","        return scores, sequences\n","    \n","    def _compute_scores(self, emissions, tags, mask):\n","        batch_size, seq_len = tags.shape\n","        if(torch.cuda.is_available()):\n","            scores = torch.zeros(batch_size).cuda()\n","        else:\n","            scores = torch.zeros(batch_size)\n","        # save first and last tags for later\n","        first_tags = tags[:, 0]\n","        last_valid_idx = mask.int().sum(1) - 1\n","        last_tags = tags.gather(1, last_valid_idx.unsqueeze(1)).squeeze()\n","        # add transition from SOS to first tags for each sample in batch\n","        t_scores = self.transitions[self.SOS_TAG_IDX, first_tags]\n","        # add emission scores of the first tag for each sample in batch\n","        e_scores = emissions[:, 0].gather(1, first_tags.unsqueeze(1)).squeeze()\n","        scores += e_scores + t_scores\n","        # repeat for every remaining word\n","        for i in range(1, seq_len):\n","            is_valid = mask[:, i]\n","            prev_tags = tags[:, i - 1]\n","            curr_tags = tags[:, i]\n","            e_scores = emissions[:, i].gather(1, curr_tags.unsqueeze(1)).squeeze()\n","            t_scores = self.transitions[prev_tags, curr_tags]\n","            # apply the mask\n","            e_scores = e_scores * is_valid\n","            t_scores = t_scores * is_valid\n","            scores += e_scores + t_scores\n","        # add transition from last tag to EOS for each sample in batch\n","        scores += self.transitions[last_tags, self.EOS_TAG_IDX]\n","        return scores\n","    \n","    # compute the partition function in log-space using forward algorithm\n","    def _compute_log_partition(self, emissions, mask):\n","        batch_size, seq_len, n_tags = emissions.shape\n","        # in the first step, SOS has all the scores\n","        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n","        for i in range(1, seq_len):\n","            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n","            e_scores = emissions[:, i].unsqueeze(1) \n","            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n","            t_scores = self.transitions.unsqueeze(0)\n","            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n","            a_scores = alphas.unsqueeze(2)\n","            scores = e_scores + t_scores + a_scores\n","            new_alphas = torch.logsumexp(scores, dim = 1)\n","            # set alphas if the mask is valid, else keep current values\n","            is_valid = mask[:, i].unsqueeze(-1)\n","            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n","        # add scores for final transition\n","        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n","        end_scores = alphas + last_transition.unsqueeze(0)\n","        # return log_sum_exp\n","        return torch.logsumexp(end_scores, dim = 1)\n","    \n","    # return a list of optimal tag sequence for each example in the batch\n","    def _viterbi_decode(self, emissions, mask):\n","        batch_size, seq_len, n_tags = emissions.shape\n","        # in the first iteration, SOS will have all the scores and then, the max\n","        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n","        backpointers = []\n","        for i in range(1, seq_len):\n","            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n","            e_scores = emissions[:, i].unsqueeze(1) \n","            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n","            t_scores = self.transitions.unsqueeze(0)\n","            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n","            a_scores = alphas.unsqueeze(2)\n","            scores = e_scores + t_scores + a_scores\n","            # find the highest score and tag, instead of log_sum_exp\n","            max_scores, max_score_tags = torch.max(scores, dim = 1)\n","            # set alphas if the mask is valid, otherwise keep the current values\n","            is_valid = mask[:, i].unsqueeze(-1)\n","            alphas = is_valid * max_scores + (1 - is_valid) * alphas\n","            backpointers.append(max_score_tags.t())\n","        # add scores for final transition\n","        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n","        end_scores = alphas + last_transition.unsqueeze(0)\n","        # get the final most probable score and the final most probable tag\n","        max_final_scores, max_final_tags = torch.max(end_scores, dim=1)\n","        # find the best sequence of labels for each sample in the batch\n","        best_sequences = []\n","        emission_lengths = mask.int().sum(dim=1)\n","        for i in range(batch_size):\n","            # recover the original sentence length for the i-th sample in the batch\n","            sample_length = emission_lengths[i].item()\n","            # recover the max tag for the last timestep\n","            sample_final_tag = max_final_tags[i].item()\n","            # limit the backpointers until the last but one\n","            # since the last corresponds to the sample_final_tag\n","            sample_backpointers = backpointers[: sample_length - 1]\n","            # follow the backpointers to build the sequence of labels\n","            sample_path = self._find_best_path(i, sample_final_tag, sample_backpointers)\n","            # add this path to the list of best sequences\n","            best_sequences.append(sample_path)\n","        return max_final_scores, best_sequences\n","    \n","    # auxiliary function to find the best path sequence for a specific example\n","    def _find_best_path(self, sample_id, best_tag, backpointers):\n","        ## backpointers: list[tensor[seq_len_i - 1, n_tags, batch_size]], seq_len_i is the length of the i-th sample of the batch\n","        # add the final best_tag to our best path\n","        best_path = [best_tag]\n","\n","        # traverse the backpointers in backwards\n","        for backpointers_t in reversed(backpointers):\n","\n","            # recover the best_tag at this timestep\n","            best_tag = backpointers_t[best_tag][sample_id].item()\n","\n","            # append to the beginning of the list so we don't need to reverse it later\n","            best_path.insert(0, best_tag)\n","        return best_path"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["MTL Model to classify. Our Architecture which used the RR component and \n","Shift component parallely to get the emission scores and then they are\n","fed into the CRF to get the appropriate probabilities for each label.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Hier_LSTM_CRF_Classifier(nn.Module):\n","    def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, pad_word_idx = 0, pretrained = False, device = 'cuda'):\n","        super().__init__()\n","        \n","        self.emb_dim = sent_emb_dim\n","        self.pretrained = pretrained\n","        self.device = device\n","        self.pad_tag_idx = pad_tag_idx\n","        self.pad_word_idx = pad_word_idx\n","        \n","        ## RR Modele    \n","        self.emitter = LSTM_Emitter(n_tags, 3*sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n","        self.crf = CRF(n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n","        \n","        ## Shift or Binary Module\n","        self.emitter_binary = LSTM_Emitter_Binary(5, 3*sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n","        self.crf_binary = CRF(5, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n","        \n","    \n","    def forward(self, x, x_binary):\n","        batch_size = len(x)\n","        seq_lengths = [len(doc) for doc in x]\n","        max_seq_len = max(seq_lengths)\n","        \n","            \n","        ## x: list[batch_size, sents_per_doc, sent_emb_dim]\n","        tensor_x = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x]\n","        tensor_x_binary = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x_binary]\n","        \n","        ## list[batch_size, sents_per_doc, sent_emb_dim] --> tensor[batch_size, max_seq_len, sent_emb_dim]\n","        tensor_x = nn.utils.rnn.pad_sequence(tensor_x, batch_first = True).to(self.device)    \n","        tensor_x_binary = nn.utils.rnn.pad_sequence(tensor_x_binary, batch_first = True).to(self.device)  \n","        \n","        self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n","        for i, sl in enumerate(seq_lengths):\n","            self.mask[i, :sl] = 1\n","        \n","        ## Get hidden states of Shift Module and pass them to the RR Module for emission score calculation for RR Module\n","        self.emissions_binary, self.hidden_binary = self.emitter_binary(tensor_x_binary)\n","        self.emissions = self.emitter(tensor_x, self.hidden_binary)\n","        \n","        ## Passing the emission scores to the CRF to get the final sequence of tags\n","        _, path = self.crf.decode(self.emissions, mask = self.mask)\n","        _, path_binary = self.crf_binary.decode(self.emissions_binary, mask = self.mask)\n","        return path, path_binary\n","    \n","    def _loss(self, y):\n","        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n","        tensor_y = [torch.tensor(doc, dtype = torch.long) for doc in y]\n","        tensor_y = nn.utils.rnn.pad_sequence(tensor_y, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n","        \n","        nll = self.crf(self.emissions, tensor_y, mask = self.mask)\n","        return nll    \n","    \n","    def _loss_binary(self, y_binary):\n","        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n","        tensor_y_binary = [torch.tensor(doc, dtype = torch.long) for doc in y_binary]\n","        tensor_y_binary = nn.utils.rnn.pad_sequence(tensor_y_binary, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n","        \n","        nll_binary = self.crf_binary(self.emissions_binary, tensor_y_binary, mask = self.mask)\n","        return nll_binary    "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","<br>\n","    Top-level module which uses a Hierarchical-LSTM-CRF to classify.<br>\n","    Sentence embeddings are then passed to LSTM_Emitter to generate emission scores, <br>\n","    and finally CRF is used to obtain optimal tag sequence. <br>\n","    Emission scores are fed to the CRF to generate optimal tag sequence.<br>\n","<br>\n","class Hier_LSTM_CRF_Classifier(nn.Module):<br>\n","    def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, word_emb_dim = 0, pad_word_idx = 0, pretrained = False, device = 'cuda'):<br>\n","        super().__init__()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# '''Top-level module which uses a Hierarchical-LSTM-CRF to classify.\n","# Sentence embeddings are then passed to LSTM_Emitter to generate emission scores,\n","# and finally CRF is used to obtain optimal tag sequence.\n","# Emission scores are fed to the CRF to generate optimal tag sequence.'''\n","# class Hier_LSTM_CRF_Classifier(nn.Module):\n","#     def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, word_emb_dim = 0, pad_word_idx = 0, pretrained = False, device = 'cuda'):\n","#         super().__init__()        \n","#         self.emb_dim = sent_emb_dim\n","#         self.pretrained = pretrained\n","#         self.device = device\n","#         self.pad_tag_idx = pad_tag_idx\n","#         self.pad_word_idx = pad_word_idx\n","            \n","#         self.emitter = LSTM_Emitter(n_tags, sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n","#         self.crf = CRF(n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n","        \n","    \n","#     def forward(self, x):\n","#         batch_size = len(x)\n","#         seq_lengths = [len(doc) for doc in x]\n","#         max_seq_len = max(seq_lengths)\n","        \n","#         ## x: list[batch_size, sents_per_doc, sent_emb_dim]\n","#         tensor_x = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x]\n","        \n","#         ## list[batch_size, sents_per_doc, sent_emb_dim] --> tensor[batch_size, max_seq_len, sent_emb_dim]\n","#         tensor_x = nn.utils.rnn.pad_sequence(tensor_x, batch_first = True).to(self.device)        \n","        \n","#         self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n","#         for i, sl in enumerate(seq_lengths):\n","#             self.mask[i, :sl] = 1\t\n","        \n","#         self.emissions = self.emitter(tensor_x)\n","#         _, path = self.crf.decode(self.emissions, mask = self.mask)\n","#         return path\n","    \n","#     def _loss(self, y):\n","#         ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n","#         tensor_y = [torch.tensor(doc, dtype = torch.long) for doc in y]\n","#         tensor_y = nn.utils.rnn.pad_sequence(tensor_y, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n","        \n","#         nll = self.crf(self.emissions, tensor_y, mask = self.mask)\n","#         return nll    "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Loading.... tag2idx\")\n","tag2idx = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"Fact\": 3, \"RulingByLowerCourt\": 4, \"Argument\": 5, \"Statute\": 6, \"RatioOfTheDecision\": 7, \"RulingByPresentCourt\": 8, \"Precedent\": 9,\"Dissent\": 10}\n","idx2tag = {v: k for k, v in tag2idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Loading....rr model\")\n","save_path = \"all_models/saved_new/\"\n","emb_dim = 768\n","device_type='cuda'\n","model_best = Hier_LSTM_CRF_Classifier(len(tag2idx), emb_dim, tag2idx['<start>'], tag2idx['<end>'], tag2idx['<pad>'], vocab_size = 2, pretrained = True, device = device_type).to(device_type)\n","model_state = torch.load(save_path + 'model_state.tar')\n","# print(\"Printing keys in model_state..........\")\n","# print(model_state.keys())\n","model_best.load_state_dict(model_state['state_dict'])\n","print(\" RR Model Load Successful\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Generate_embedding section (saves embeddings in json files)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Inside Generate_embedding..........\")\n","tokenizer_gen_emb = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n","model_gen_emb = BertModel.from_pretrained(\"bert-base-uncased\")\n","model_gen_emb.eval() # Setting model to evaluation mode\n","print('cuda' if torch.cuda.is_available() else 'cpu')\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","model_gen_emb.to(device)\n","print(\"Gen emb model loading sucessful......\")\n","# print(\"Exiting the code.....\")\n","# exit()\n","print(\"***************************Starting the actual processing**************************\")\n","file_list = os.listdir(raw_input_path) #list of all sav files\n","print(\"Number of sav files in the location \",raw_input_path, \" is \",len(file_list))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for file_name in tqdm(file_list, desc=\"sav files:\"):\n","    file_path = raw_input_path + file_name\n","    print(\"Loading file:\", file_name)\n","    with open(file_path, \"rb\") as f:\n","        train_it = pkl.load(f)\n","     \n","    it_train_files = list(train_it['candidate_data'].keys())\n","    print(\"Number of files in \",file_name,\" is: \",len(it_train_files))\n","    \n","    out_gen = emb_output_path+\"gen_emb/\"+file_name.split(\".\")[0]\n","    print(\"Creating gen embbeding output path: \",out_gen)\n","    os.makedirs(out_gen, exist_ok = True) \n","    print(\"Created gen emb output path: \",out_gen)\n","    out_bertsc = emb_output_path+\"bertsc_emb/\"+file_name.split(\".\")[0]+\"/\"\n","    print(\"Creating gen embbeding output path: \",out_bertsc)\n","    os.makedirs(out_bertsc, exist_ok = True) \n","    print(\"Created gen emb output path: \",out_bertsc)\n","    # uncomment from here\n","    # Getting the embeddings for case in train_it, similarly follow for other files\n","    # Here we use the embedding corresponding to the [CLS] token as the sentences representation\n","    for case in tqdm(it_train_files, desc=\"Generate Emb:\"):\n","        sentences = train_it['candidate_data'][case]\n","        all_text = {}\n","        start_time = time.time()\n","        for idx in range(len(sentences)):\n","            text = sentences[idx]\n","            marked_text = \"[CLS] \" + text + \" [SEP]\"\n","            tokenized_text = tokenizer_gen_emb.tokenize(marked_text)\n","            if(len(tokenized_text) > 510):\n","                tokenized_text = tokenized_text[:510] + ['[SEP]']\n","        \n","            indexed_tokens = tokenizer_gen_emb.convert_tokens_to_ids(tokenized_text)\n","            segments_ids = [1] * len(tokenized_text)\n","            tokens_tensor = torch.tensor([indexed_tokens]).to(device)\n","            segments_tensors = torch.tensor([segments_ids]).to(device)\n","            \n","            with torch.no_grad():\n","                outputs = model_gen_emb(tokens_tensor, segments_tensors)\n","            \n","            emb = outputs[0].squeeze()[0].flatten().tolist()\n","            \n","            emb = [str(round(i,5)) for i in emb]\n","            all_text[idx] = emb\n","        json_object = json.dumps(all_text, indent=4)\n","        with open(os.path.join(out_gen, str(case)+\".json\"),\"w\") as f:\n","            f.write(json_object)\n","    print(\"***********Finished Generating embeddings for docs in \",file_name,\" ************\")\n","\n","    #Creating BERTSC embeddings\n","    print(\"***********Starting BERTSC embeddings for docs in \",file_name,\" ************\")\n","    data_tr_CL = train_it['candidate_data']\n","    train_df_CL = json_to_df(data_tr_CL)\n","    train_input_ids, train_lengths, train_token_type_ids = input_id_maker(train_df_CL, tokenizer_bertsc)\n","    train_attention_masks = att_masking(train_input_ids)\n","    # Getting embeddings for sentences\n","    clsembs_train = []\n","    for i in tqdm(range(len(train_input_ids)), desc=\"BertSC Embeddings\"):\n","        ii = train_input_ids[i]\n","        clsembs_train.append(get_output_for_one_vec(ii, train_attention_masks[i]))\n","    \n","    #Saving the embeddings\n","    i=0 \n","    for key in tqdm(it_train_files, desc=\"Saving BertSC Embs:\"):\n","        limit = len(data_tr_CL[key])\n","        sp = clsembs_train[i:i+limit-1]\n","        np.save(out_bertsc+ str(key), np.array(sp))\n","        i = i+limit-1\n","    \n","    print(\"************Finished BERTSC embeddings for docs in \",file_name,\" *************\")\n","\n","    #Rhetorical Roles Results - RRR\n","    rr_results = dict()\n","    print(\":::::::::::::::::Starting Rhetorical Roles Results for \", file_name ,\"::::::::::::::::\")\n","    file_rr_dir = file_rr_output_path+file_name.split(\".\")[0]+\"/\"\n","    os.makedirs(file_rr_dir, exist_ok = True)\n","    print(\"len of it_train_files:\",len(it_train_files))\n","    for i in tqdm(range(len(it_train_files)), desc= \"Rhetorical Roles: \"):\n","        case_id = it_train_files[i]\n","        sentences = train_it['candidate_data'][case_id]\n","        file_gen_emb = out_gen+\"/\"+str(case_id)+\".json\"\n","        file_bertsc_emb  = out_bertsc+str(case_id)+\".npy\"\n","        batch_x_binary = get_s_emb(file_gen_emb,file_bertsc_emb) # 3*768 size emb\n","        batch_x_binary = batch_x_binary.reshape((1,batch_x_binary.shape[0], batch_x_binary.shape[1]))\n","        #print(\"For \",file_name,\" batch_x_binary shape: \",batch_x_binary.shape)\n","        #print(\"Predicting labels for \",file_name,\"........\")\n","        pred, pred_binary = model_best(batch_x_binary, batch_x_binary)\n","        # #print(\"Prediction Successful\")\n","        #Convert tag_ids to tags\n","        tag_lst = [idx2tag[idx] for idx in pred[0]]\n","\n","        # create list of pairs for sentence and its RR\n","        #save list corresponding to doc id as key-value pair in dict\n","        rr_results[case_id] = list(zip(sentences,tag_lst))\n","        with open(file_rr_dir+str(case_id)+\".json\",\"w\") as f:\n","            f.write(json.dumps({case_id:list(zip(sentences,tag_lst))},indent=4))\n","    json_object = json.dumps(rr_results, indent=4)\n","    with open(rr_output_path+file_name.split(\".\")[0]+\"_rr_results\"+\".json\",\"w\") as f:\n","        f.write(json_object)\n","    print(\"************Finished Rhetorical Roles for docs in \",file_name,\" *************\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"Total Processing Finished... check \",rr_output_path,\" for the results\")\n","print(\"ciao!!\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
