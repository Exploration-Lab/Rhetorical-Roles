{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"## Imports\n\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pack_sequence, pack_padded_sequence, pad_packed_sequence\nimport time\nimport json\nimport random\nimport numpy as np\nfrom sklearn.metrics import precision_score, recall_score, f1_score, classification_report\nimport string\nfrom collections import defaultdict\nimport os\n\nSEED = 42\n\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\n","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:30.926785Z","iopub.execute_input":"2021-07-12T18:43:30.927083Z","iopub.status.idle":"2021-07-12T18:43:32.830009Z","shell.execute_reply.started":"2021-07-12T18:43:30.927052Z","shell.execute_reply":"2021-07-12T18:43:32.829176Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    A Bi-LSTM is used to generate feature vectors for each sentence from the sentence embeddings. \n    The feature vectors are actually context-aware sentence embeddings. These are then fed to a \n    feed-forward network to obtain emission scores for each class at each sentence.\n'''\nclass LSTM_Emitter(nn.Module):\n    def __init__(self, n_tags, emb_dim, hidden_dim, drop = 0.5, device = 'cuda'):\n        super().__init__()\n        \n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(emb_dim, hidden_dim // 2, bidirectional = True, batch_first = True)\n        self.dropout = nn.Dropout(drop)\n        self.hidden2tag = nn.Linear(hidden_dim, n_tags)\n        self.hidden = None\n        self.device = device\n        \n    def init_hidden(self, batch_size):\n        return (torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device), torch.randn(2, batch_size, self.hidden_dim // 2).to(self.device))\n    \n    def forward(self, sequences):\n        ## sequences: tensor[batch_size, max_seq_len, emb_dim]\n        \n        # initialize hidden state\n        self.hidden = self.init_hidden(sequences.shape[0])\n        \n        # generate context-aware sentence embeddings (feature vectors)\n        ## tensor[batch_size, max_seq_len, emb_dim] --> tensor[batch_size, max_seq_len, hidden_dim]\n        x, self.hidden = self.lstm(sequences, self.hidden)\n        x = self.dropout(x)\n        \n        # generate emission scores for each class at each sentence\n        # tensor[batch_size, max_seq_len, hidden_dim] --> tensor[batch_size, max_seq_len, n_tags]\n        x = self.hidden2tag(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:32.833419Z","iopub.execute_input":"2021-07-12T18:43:32.833683Z","iopub.status.idle":"2021-07-12T18:43:32.844145Z","shell.execute_reply.started":"2021-07-12T18:43:32.833658Z","shell.execute_reply":"2021-07-12T18:43:32.843291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    A linear-chain CRF is fed with the emission scores at each sentence, \n    and it finds out the optimal sequence of tags by learning the transition scores.\n'''\nclass CRF(nn.Module):    \n    def __init__(self, n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx = None):\n        super().__init__()\n        \n        self.n_tags = n_tags\n        self.SOS_TAG_IDX = sos_tag_idx\n        self.EOS_TAG_IDX = eos_tag_idx\n        self.PAD_TAG_IDX = pad_tag_idx\n        \n        self.transitions = nn.Parameter(torch.empty(self.n_tags, self.n_tags))\n        self.init_weights()\n        \n    def init_weights(self):\n        # initialize transitions from random uniform distribution between -0.1 and 0.1\n        nn.init.uniform_(self.transitions, -0.1, 0.1)\n        \n        # enforce constraints (rows = from, cols = to) with a big negative number.\n        # exp(-1000000) ~ 0\n        \n        # no transitions to SOS\n        self.transitions.data[:, self.SOS_TAG_IDX] = -1000000.0\n        # no transition from EOS\n        self.transitions.data[self.EOS_TAG_IDX, :] = -1000000.0\n        \n        if self.PAD_TAG_IDX is not None:\n            # no transitions from pad except to pad\n            self.transitions.data[self.PAD_TAG_IDX, :] = -1000000.0\n            self.transitions.data[:, self.PAD_TAG_IDX] = -1000000.0\n            # transitions allowed from end and pad to pad\n            self.transitions.data[self.PAD_TAG_IDX, self.EOS_TAG_IDX] = 0.0\n            self.transitions.data[self.PAD_TAG_IDX, self.PAD_TAG_IDX] = 0.0\n            \n    def forward(self, emissions, tags, mask = None):\n        ## emissions: tensor[batch_size, seq_len, n_tags]\n        ## tags: tensor[batch_size, seq_len]\n        ## mask: tensor[batch_size, seq_len], indicates valid positions (0 for pad)\n        return -self.log_likelihood(emissions, tags, mask = mask)\n    \n    def log_likelihood(self, emissions, tags, mask = None):                   \n        if mask is None:\n            mask = torch.ones(emissions.shape[:2])\n            \n        scores = self._compute_scores(emissions, tags, mask = mask)\n        partition = self._compute_log_partition(emissions, mask = mask)\n        return torch.sum(scores - partition)\n    \n    # find out the optimal tag sequence using Viterbi Decoding Algorithm\n    def decode(self, emissions, mask = None):      \n        if mask is None:\n            mask = torch.ones(emissions.shape[:2])\n            \n        scores, sequences = self._viterbi_decode(emissions, mask)\n        return scores, sequences\n    \n    def _compute_scores(self, emissions, tags, mask):\n        batch_size, seq_len = tags.shape\n        if(torch.cuda.is_available()):\n            scores = torch.zeros(batch_size).cuda()\n        else:\n            scores = torch.zeros(batch_size)\n        \n        # save first and last tags for later\n        first_tags = tags[:, 0]\n        last_valid_idx = mask.int().sum(1) - 1\n        last_tags = tags.gather(1, last_valid_idx.unsqueeze(1)).squeeze()\n        \n        # add transition from SOS to first tags for each sample in batch\n        t_scores = self.transitions[self.SOS_TAG_IDX, first_tags]\n        \n        # add emission scores of the first tag for each sample in batch\n        e_scores = emissions[:, 0].gather(1, first_tags.unsqueeze(1)).squeeze()\n        scores += e_scores + t_scores\n        \n        # repeat for every remaining word\n        for i in range(1, seq_len):\n            \n            is_valid = mask[:, i]\n            prev_tags = tags[:, i - 1]\n            curr_tags = tags[:, i]\n            \n            e_scores = emissions[:, i].gather(1, curr_tags.unsqueeze(1)).squeeze()\n            t_scores = self.transitions[prev_tags, curr_tags]\n                        \n            # apply the mask\n            e_scores = e_scores * is_valid\n            t_scores = t_scores * is_valid\n            \n            scores += e_scores + t_scores\n            \n        # add transition from last tag to EOS for each sample in batch\n        scores += self.transitions[last_tags, self.EOS_TAG_IDX]\n        return scores\n    \n    # compute the partition function in log-space using forward algorithm\n    def _compute_log_partition(self, emissions, mask):\n        batch_size, seq_len, n_tags = emissions.shape\n        \n        # in the first step, SOS has all the scores\n        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n        \n        for i in range(1, seq_len):\n            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n            e_scores = emissions[:, i].unsqueeze(1) \n            \n            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n            t_scores = self.transitions.unsqueeze(0)\n            \n            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n            a_scores = alphas.unsqueeze(2)\n            \n            scores = e_scores + t_scores + a_scores\n            new_alphas = torch.logsumexp(scores, dim = 1)\n            \n            # set alphas if the mask is valid, else keep current values\n            is_valid = mask[:, i].unsqueeze(-1)\n            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n            \n        # add scores for final transition\n        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n        end_scores = alphas + last_transition.unsqueeze(0)\n        \n        # return log_sum_exp\n        return torch.logsumexp(end_scores, dim = 1)\n    \n    # return a list of optimal tag sequence for each example in the batch\n    def _viterbi_decode(self, emissions, mask):\n        batch_size, seq_len, n_tags = emissions.shape\n        \n        # in the first iteration, SOS will have all the scores and then, the max\n        alphas = self.transitions[self.SOS_TAG_IDX, :].unsqueeze(0) + emissions[:, 0]\n        \n        backpointers = []\n        \n        for i in range(1, seq_len):\n            ## tensor[batch_size, n_tags] -> tensor[batch_size, 1, n_tags]\n            e_scores = emissions[:, i].unsqueeze(1) \n            \n            ## tensor[n_tags, n_tags] -> tensor[batch_size, n_tags, n_tags]\n            t_scores = self.transitions.unsqueeze(0)\n            \n            ## tensor[batch_size, n_tags] -> tensor[batch_size, n_tags, 1]\n            a_scores = alphas.unsqueeze(2)\n            \n            scores = e_scores + t_scores + a_scores\n            \n            # find the highest score and tag, instead of log_sum_exp\n            max_scores, max_score_tags = torch.max(scores, dim = 1)\n            \n            # set alphas if the mask is valid, otherwise keep the current values\n            is_valid = mask[:, i].unsqueeze(-1)\n            alphas = is_valid * max_scores + (1 - is_valid) * alphas\n            \n            backpointers.append(max_score_tags.t())\n            \n        # add scores for final transition\n        last_transition = self.transitions[:, self.EOS_TAG_IDX]\n        end_scores = alphas + last_transition.unsqueeze(0)\n\n        # get the final most probable score and the final most probable tag\n        max_final_scores, max_final_tags = torch.max(end_scores, dim=1)\n\n        # find the best sequence of labels for each sample in the batch\n        best_sequences = []\n        emission_lengths = mask.int().sum(dim=1)\n        for i in range(batch_size):\n\n            # recover the original sentence length for the i-th sample in the batch\n            sample_length = emission_lengths[i].item()\n\n            # recover the max tag for the last timestep\n            sample_final_tag = max_final_tags[i].item()\n\n            # limit the backpointers until the last but one\n            # since the last corresponds to the sample_final_tag\n            sample_backpointers = backpointers[: sample_length - 1]\n\n            # follow the backpointers to build the sequence of labels\n            sample_path = self._find_best_path(i, sample_final_tag, sample_backpointers)\n\n            # add this path to the list of best sequences\n            best_sequences.append(sample_path)\n\n        return max_final_scores, best_sequences\n    \n    # auxiliary function to find the best path sequence for a specific example\n    def _find_best_path(self, sample_id, best_tag, backpointers):\n        ## backpointers: list[tensor[seq_len_i - 1, n_tags, batch_size]], seq_len_i is the length of the i-th sample of the batch\n        \n        # add the final best_tag to our best path\n        best_path = [best_tag]\n\n        # traverse the backpointers in backwards\n        for backpointers_t in reversed(backpointers):\n\n            # recover the best_tag at this timestep\n            best_tag = backpointers_t[best_tag][sample_id].item()\n\n            # append to the beginning of the list so we don't need to reverse it later\n            best_path.insert(0, best_tag)\n\n        return best_path","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:32.845839Z","iopub.execute_input":"2021-07-12T18:43:32.846133Z","iopub.status.idle":"2021-07-12T18:43:32.962236Z","shell.execute_reply.started":"2021-07-12T18:43:32.846109Z","shell.execute_reply":"2021-07-12T18:43:32.961401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    Top-level module which uses a Hierarchical-LSTM-CRF to classify.\n    Sentence embeddings are then passed to LSTM_Emitter to generate emission scores, \n    and finally CRF is used to obtain optimal tag sequence. \n    Emission scores are fed to the CRF to generate optimal tag sequence.\n'''\nclass Hier_LSTM_CRF_Classifier(nn.Module):\n    def __init__(self, n_tags, sent_emb_dim, sos_tag_idx, eos_tag_idx, pad_tag_idx, vocab_size = 0, word_emb_dim = 0, pad_word_idx = 0, pretrained = False, device = 'cuda'):\n        super().__init__()\n        \n        self.emb_dim = sent_emb_dim\n        self.pretrained = pretrained\n        self.device = device\n        self.pad_tag_idx = pad_tag_idx\n        self.pad_word_idx = pad_word_idx\n            \n        self.emitter = LSTM_Emitter(n_tags, sent_emb_dim, sent_emb_dim, 0.5, self.device).to(self.device)\n        self.crf = CRF(n_tags, sos_tag_idx, eos_tag_idx, pad_tag_idx).to(self.device)\n        \n    \n    def forward(self, x):\n        batch_size = len(x)\n        seq_lengths = [len(doc) for doc in x]\n        max_seq_len = max(seq_lengths)\n        \n        ## x: list[batch_size, sents_per_doc, sent_emb_dim]\n        tensor_x = [torch.tensor(doc, dtype = torch.float, requires_grad = True) for doc in x]\n        \n        ## list[batch_size, sents_per_doc, sent_emb_dim] --> tensor[batch_size, max_seq_len, sent_emb_dim]\n        tensor_x = nn.utils.rnn.pad_sequence(tensor_x, batch_first = True).to(self.device)        \n        \n        self.mask = torch.zeros(batch_size, max_seq_len).to(self.device)\n        for i, sl in enumerate(seq_lengths):\n            self.mask[i, :sl] = 1\t\n        \n        self.emissions = self.emitter(tensor_x)\n        _, path = self.crf.decode(self.emissions, mask = self.mask)\n        return path\n    \n    def _loss(self, y):\n        ##  list[batch_size, sents_per_doc] --> tensor[batch_size, max_seq_len]\n        tensor_y = [torch.tensor(doc, dtype = torch.long) for doc in y]\n        tensor_y = nn.utils.rnn.pad_sequence(tensor_y, batch_first = True, padding_value = self.pad_tag_idx).to(self.device)\n        \n        nll = self.crf(self.emissions, tensor_y, mask = self.mask)\n        return nll    ","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:32.963651Z","iopub.execute_input":"2021-07-12T18:43:32.964027Z","iopub.status.idle":"2021-07-12T18:43:32.977970Z","shell.execute_reply.started":"2021-07-12T18:43:32.963990Z","shell.execute_reply":"2021-07-12T18:43:32.977198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    This function prepares the numericalized data in the form of lists, to be used for training, test and evaluation.\n        x:  list[num_docs, sentences_per_doc, sentence_embedding_dim] \n        y:  list[num_docs, sentences_per_doc]\n'''\ndef prepare_data_new(idx_order_train, idx_order_val, idx_order_test, args, path_train, path_val, path_test, dim, tag2idx=None):\n    x_train, y_train = [], []\n    x_test, y_test = [], []\n    x_val, y_val = [], []\n    \n    \n    word2idx = defaultdict(lambda: len(word2idx))\n    if tag2idx is None:\n        tag2idx = defaultdict(lambda: len(tag2idx))\n        tag2idx['<pad>'], tag2idx['<start>'], tag2idx['<end>'] = 0, 1, 2\n    \n\n    # map the special symbols first\n    word2idx['<pad>'], word2idx['<unk>'] = 0, 1\n\n    # iterate over documents\n    for doc in idx_order_train:\n        doc_x, doc_y = [], [] \n\n        with open(path_train + doc) as fp:\n            \n            # iterate over sentences\n            for sent in fp:\n                try:\n                \tsent_x, sent_y = sent.strip().split('\\t')\n                except ValueError:\n                \tcontinue\n\n                # cleanse text, map words and tags\n                if not args.pretrained:\n                    sent_x = sent_x.strip().lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n                    sent_x = list(map(lambda x: word2idx[x], sent_x.split()))\n                else:\n                    sent_x = list(map(float, sent_x.strip().split()[:dim]))\n                sent_y = tag2idx[sent_y.strip()]\n\n                if sent_x != []:\n                    doc_x.append(sent_x)\n                    doc_y.append(sent_y)\n        \n        x_train.append(doc_x)\n        y_train.append(doc_y)\n   \n\n    for doc in idx_order_val:\n        doc_x, doc_y = [], [] \n\n        with open(path_val + doc) as fp:\n            \n            # iterate over sentences\n            for sent in fp:\n                try:\n                \tsent_x, sent_y = sent.strip().split('\\t')\n                except ValueError:\n                \tcontinue\n\n                # cleanse text, map words and tags\n                if not args.pretrained:\n                    sent_x = sent_x.strip().lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n                    sent_x = list(map(lambda x: word2idx[x], sent_x.split()))\n                else:\n                    sent_x = list(map(float, sent_x.strip().split()[:dim]))\n                sent_y = tag2idx[sent_y.strip()]\n\n                if sent_x != []:\n                    doc_x.append(sent_x)\n                    doc_y.append(sent_y)\n        \n        x_val.append(doc_x)\n        y_val.append(doc_y)\n\n\n    for doc in idx_order_test:\n        doc_x, doc_y = [], [] \n\n        with open(path_test + doc) as fp:\n            \n            # iterate over sentences\n            for sent in fp:\n                try:\n                \tsent_x, sent_y = sent.strip().split('\\t')\n                except ValueError:\n                \tcontinue\n\n                # cleanse text, map words and tags\n                if not args.pretrained:\n                    sent_x = sent_x.strip().lower().translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation)))\n                    sent_x = list(map(lambda x: word2idx[x], sent_x.split()))\n                else:\n                    sent_x = list(map(float, sent_x.strip().split()[:dim]))\n                sent_y = tag2idx[sent_y.strip()]\n\n                if sent_x != []:\n                    doc_x.append(sent_x)\n                    doc_y.append(sent_y)\n        \n        x_test.append(doc_x)\n        y_test.append(doc_y)\n\n    return x_train, y_train, x_val, y_val, x_test, y_test, word2idx, tag2idx","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:32.979366Z","iopub.execute_input":"2021-07-12T18:43:32.979720Z","iopub.status.idle":"2021-07-12T18:43:32.998377Z","shell.execute_reply.started":"2021-07-12T18:43:32.979686Z","shell.execute_reply":"2021-07-12T18:43:32.997387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''\n    This function is used to divide out data into batches of sizes batch_size\n'''\ndef batchify(x, y, batch_size):\n    idx = list(range(len(x)))\n    random.shuffle(idx)\n    \n    # convert to numpy array for ease of indexing\n    x = np.array(x)[idx]\n    y = np.array(y)[idx]\n    \n    i = 0\n    while i < len(x):\n        j = min(i + batch_size, len(x))\n        \n        batch_idx = idx[i : j]\n        batch_x = x[i : j]\n        batch_y = y[i : j]\n        \n        yield batch_idx, batch_x, batch_y\n        \n        i = j\n\n\n'''\n    Perform a single training step by iterating over the entire training data once. Data is divided into batches.\n'''\ndef train_step(model, opt, x, y, batch_size):\n    ## x: list[num_examples, sents_per_example, features_per_sentence]\n    ## y: list[num_examples, sents_per_example]\n    \n    model.train()\n    \n    total_loss = 0\n    y_pred = [] # predictions\n    y_gold = [] # gold standard\n    idx = [] # example index\n    \n    for i, (batch_idx, batch_x, batch_y) in enumerate(batchify(x, y, batch_size)):\n        pred = model(batch_x)\n        loss = model._loss(batch_y)        \n\n        opt.zero_grad()\n        loss.backward()\n        opt.step()\n        \n        total_loss += loss.item()\n     \n        y_pred.extend(pred)\n        y_gold.extend(batch_y)\n        idx.extend(batch_idx)\n        \n    assert len(sum(y, [])) == len(sum(y_pred, [])), \"Mismatch in predicted\"\n    \n    return total_loss / (i + 1), idx, y_gold, y_pred\n\n'''\n    Perform a single evaluation step by iterating over the entire training data once. Data is divided into batches.\n'''\ndef val_step(model, x, y, batch_size):\n    ## x: list[num_examples, sents_per_example, features_per_sentence]\n    ## y: list[num_examples, sents_per_example]\n    \n    model.eval()\n    \n    total_loss = 0\n    y_pred = [] # predictions\n    y_gold = [] # gold standard\n    idx = [] # example index\n    \n    for i, (batch_idx, batch_x, batch_y) in enumerate(batchify(x, y, batch_size)):\n        pred = model(batch_x)\n        loss = model._loss(batch_y)\n               \n        total_loss += loss.item()\n     \n        y_pred.extend(pred)\n        y_gold.extend(batch_y)\n        idx.extend(batch_idx)\n        \n    assert len(sum(y, [])) == len(sum(y_pred, [])), \"Mismatch in predicted\"\n    \n    return total_loss / (i + 1), idx, y_gold, y_pred\n\n\n'''\n    Report all metrics in format using sklearn.metrics.classification_report\n'''\ndef statistics(data_state, tag2idx):\n    idx, gold, pred = data_state['idx'], data_state['gold'], data_state['pred']\n    \n    rev_tag2idx = {v: k for k, v in tag2idx.items()}\n    tags = [rev_tag2idx[i] for i in range(len(tag2idx)) if rev_tag2idx[i] not in ['<start>', '<end>', '<pad>']]\n    \n    # flatten out\n    gold = sum(gold, [])\n    pred = sum(pred, [])\n    \n    \n    print(classification_report(gold, pred, target_names = tags, digits = 3))\n\n'''\n    Train the model on entire dataset and report loss and macro-F1 after each epoch.\n'''\ndef learn(model, train_x, train_y, val_x, val_y, test_x, test_y, tag2idx, args):\n    \n    opt = torch.optim.Adam(model.parameters(), lr = args.lr, weight_decay = args.reg)\n    \n    print(\"{0:>7}  {1:>10}  {2:>6}  {3:>10}  {4:>6}\".format('EPOCH', 'Tr_LOSS', 'Tr_F1', 'Val_LOSS', 'Val_F1'))\n    print(\"-----------------------------------------------------------\")\n    \n    best_val_f1 = 0.0\n    \n    model_state = {}\n    data_state = {}\n    \n    start_time = time.time()\n    \n    for epoch in range(1, args.epochs + 1):\n\n        train_loss, train_idx, train_gold, train_pred = train_step(model, opt, train_x, train_y, args.batch_size)\n        val_loss, val_idx, val_gold, val_pred = val_step(model, val_x, val_y, args.batch_size)\n\n        train_f1 = f1_score(sum(train_gold, []), sum(train_pred, []), average = 'macro')\n        val_f1 = f1_score(sum(val_gold, []), sum(val_pred, []), average = 'macro')\n\n        if epoch % args.print_every == 0:\n            print(\"{0:7d}  {1:10.3f}  {2:6.3f}  {3:10.3f}  {4:6.3f}\".format(epoch, train_loss, train_f1, val_loss, val_f1))\n\n        if val_f1 > best_val_f1:\n            best_val_f1 = val_f1\n            model_state = {'epoch': epoch, 'arch': model, 'name': model.__class__.__name__, 'state_dict': model.state_dict(), 'best_f1': val_f1, 'optimizer' : opt.state_dict()}\n            data_state = {'idx': val_idx, 'loss': val_loss, 'gold': val_gold, 'pred': val_pred}\n            \n    end_time = time.time()\n    \n    print(\"Dumping model and data ...\", end = ' ')\n    \n    torch.save(model_state, args.save_path + 'model_state' + '.tar')\n    \n    with open(args.save_path + 'data_state' + '.json', 'w') as fp:\n        json.dump(data_state, fp)\n    \n    print(\"Done\")    \n\n    print('Time taken:', int(end_time - start_time), 'secs')\n    \n    ## Results on val data\n    statistics(data_state, tag2idx)\n    \n    ## Getting results on test data(best model on Val)\n    model_best = Hier_LSTM_CRF_Classifier(len(tag2idx), args.emb_dim, tag2idx['<start>'], tag2idx['<end>'], tag2idx['<pad>'], vocab_size = 2, pretrained = args.pretrained, device = args.device).to(args.device)\n    model_state = torch.load('/kaggle/working/saved/model_state.tar')\n    model_best.load_state_dict(model_state['state_dict'])\n    test_loss, test_idx, test_gold, test_pred = val_step(model_best, test_x, test_y, args.batch_size)\n    data_state = {'idx': test_idx, 'loss': test_loss, 'gold': test_gold, 'pred': test_pred}\n    statistics(data_state, tag2idx)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:43:32.999829Z","iopub.execute_input":"2021-07-12T18:43:33.000222Z","iopub.status.idle":"2021-07-12T18:43:33.027347Z","shell.execute_reply.started":"2021-07-12T18:43:33.000186Z","shell.execute_reply":"2021-07-12T18:43:33.026448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Args:\n    pretrained = True\n    data_path = '/kaggle/input/it-compressed-no-none/' ## Input to the pre-trained embedding(should contain 4 sub-folders, IT test and train, CL test and train)\n    save_path = '/kaggle/working/saved/' ## path to save the model\n    device = 'cuda' ## device to be used\n    batch_size = 40 ## batch size\n    print_every = 5 ## print loss after these many epochs\n    lr = 0.01 ## learning rate\n    reg = 0 ## weight decay for Adam Opt\n    emb_dim = 200 ## the pre-trained embedding dimension of the sentences\n    epochs = 300 ## Something between 250-300","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:44:39.005431Z","iopub.execute_input":"2021-07-12T18:44:39.005755Z","iopub.status.idle":"2021-07-12T18:44:39.010373Z","shell.execute_reply.started":"2021-07-12T18:44:39.005725Z","shell.execute_reply":"2021-07-12T18:44:39.009314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"args = Args()\nnp.warnings.filterwarnings('ignore', category=np.VisibleDeprecationWarning) \n\n## creating a directory to save models and other utility files\n!mkdir '/kaggle/working/saved/'","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:44:40.492221Z","iopub.execute_input":"2021-07-12T18:44:40.492545Z","iopub.status.idle":"2021-07-12T18:44:40.496660Z","shell.execute_reply.started":"2021-07-12T18:44:40.492513Z","shell.execute_reply":"2021-07-12T18:44:40.495786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## This cell just given the training and test files.\nimport json\n\nIT_train_path = '/kaggle/input/rhetorical-dataset/IT_train.json'\nIT_test_path = '/kaggle/input/rhetorical-dataset/IT_test.json'\nCL_train_path = '/kaggle/input/rhetorical-dataset/CL_train.json'\nCL_test_path = '/kaggle/input/rhetorical-dataset/CL_test.json'\n\ntrain_it = {}\ntest_it = {}\ntrain_cl = {}\ntest_cl = {}\n\nwith open(IT_train_path, 'r') as f:\n    train_it = json.load(f)\n    f.close()\n    \nwith open(IT_test_path, 'r') as f:\n    test_it = json.load(f)\n    f.close()\n\nwith open(CL_train_path, 'r') as f:\n    train_cl = json.load(f)\n    f.close()\n\nwith open(CL_test_path, 'r') as f:\n    test_cl = json.load(f)\n    f.close()\n\n\nit_train_files = list(train_it.keys())\ncl_train_files = list(train_cl.keys())\nit_test_files = list(test_it.keys())\ncl_test_files = list(test_cl.keys())","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:45:19.034596Z","iopub.execute_input":"2021-07-12T18:45:19.034946Z","iopub.status.idle":"2021-07-12T18:45:19.326404Z","shell.execute_reply.started":"2021-07-12T18:45:19.034912Z","shell.execute_reply":"2021-07-12T18:45:19.325565Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## These files will be used for validation \n\nit_val_files = ['SC_2009_1357.txt',\n 'BHC_BomHC_2007_1278.txt',\n 'KHC_KolHC_1980_69.txt',\n 'BHC_BomHC_2014_1227.txt',\n 'SC_2007_389.txt']\n\ncl_val_files = ['COM_Shree_Cement_Limited__vs__Builders_Association_of_TA2015171215164519311COM615014.txt',\n 'CCI_Noida_Software_Technology_Park_Ltd_vs_Star_India_PCO201808081816041524COM760948.txt',\n 'HC_State_of_Mizoram_vs_Competition_Commission_of_IndiGH201424121422183488COM227556.txt',\n 'SC_Competition Commission of India vs Bharti Airtel Limited and Ors 05122018 SC(1).txt',\n 'CCI_Western_Coalfield_Limited__vs__SSV_Coal_Carriers_PCO20152807152158246COM767401.txt']\n\n\ndef Diff(li1, li2):\n    return list(set(li1) - set(li2)) + list(set(li2) - set(li1))\n\n\nit_test_files = Diff(it_test_files, it_val_files)\ncl_test_files = Diff(cl_test_files, cl_val_files)\n\ntrain_files = it_train_files + cl_train_files\nval_files = it_val_files + cl_val_files\ntest_files = it_test_files + cl_test_files","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:45:21.727304Z","iopub.execute_input":"2021-07-12T18:45:21.727631Z","iopub.status.idle":"2021-07-12T18:45:21.733608Z","shell.execute_reply.started":"2021-07-12T18:45:21.727594Z","shell.execute_reply":"2021-07-12T18:45:21.732503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Model on IT cases ","metadata":{}},{"cell_type":"code","source":"## Preparing data and Training Model for IT cases, similarly can be run for IT+CL and CL\n\nprint('\\nPreparing data ...', end = ' ')\n\nx_it_train, y_it_train, x_it_val, y_it_val, x_it_test, y_it_test, word2idx_it, tag2idx_it = prepare_data_new(it_train_files, it_val_files, it_test_files, args, os.path.join(args.data_path,'train_it_compressed_no_none/'), os.path.join(args.data_path,'test_it_compressed_no_none/'), os.path.join(args.data_path,'test_it_compressed_no_none/'), args.emb_dim)\n\nprint('Done')\n\nprint('#Tags IT:', len(tag2idx_it))\n\nprint('Dump word2idx and tag2idx')\nwith open(args.save_path + 'word2idx.json', 'w') as fp:\n    json.dump(word2idx_it, fp)\nwith open(args.save_path + 'tag2idx.json', 'w') as fp:\n    json.dump(tag2idx_it, fp)\n\nprint('\\nInitializing model for IT ...', end = ' ')   \nmodel = Hier_LSTM_CRF_Classifier(len(tag2idx_it), args.emb_dim, tag2idx_it['<start>'], tag2idx_it['<end>'], tag2idx_it['<pad>'], vocab_size = len(word2idx_it), pretrained = args.pretrained, device = args.device).to(args.device)\nprint('Done')\n\nprint('\\nEvaluating on test...')        \nlearn(model, x_it_train, y_it_train, x_it_val, y_it_val, x_it_test, y_it_test, tag2idx_it, args)","metadata":{"execution":{"iopub.status.busy":"2021-07-12T18:47:04.240638Z","iopub.execute_input":"2021-07-12T18:47:04.240977Z","iopub.status.idle":"2021-07-12T18:53:12.364648Z","shell.execute_reply.started":"2021-07-12T18:47:04.240946Z","shell.execute_reply":"2021-07-12T18:53:12.363872Z"},"trusted":true},"execution_count":null,"outputs":[]}]}