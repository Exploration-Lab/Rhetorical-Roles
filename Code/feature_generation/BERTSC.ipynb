{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERTSiamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "szBjtfyHsSEP"
      },
      "source": [
        "## Importing required libraries\n",
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/Drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lwc4ncNDsb8z"
      },
      "source": [
        "## path for the training and testing files\n",
        "f_train_CL = open(\"/content/Drive/MyDrive/Technical/RR/Data/CL_train.json\", \"r\")\n",
        "f_test_CL = open(\"/content/Drive/MyDrive/Technical/RR/Data/CL_test.json\", \"r\")\n",
        "f_test_IT = open(\"/content/Drive/MyDrive/Technical/RR/Data/IT_test.json\", \"r\")\n",
        "f_train_IT = open(\"/content/Drive/MyDrive/Technical/RR/Data/IT_train.json\", \"r\")\n",
        "\n",
        "## A mapper to map the label to compressed labels\n",
        "label_mapper = {\n",
        "    \"Fact\": \"Fact\",\n",
        "    \"Issue\": \"Fact\",\n",
        "    \"ArgumentPetitioner\": \"Argument\",\n",
        "    \"ArgumentRespondent\": \"Argument\",\n",
        "    \"PrecedentReliedUpon\": \"Precedent\",\n",
        "    \"PrecedentNotReliedUpon\": \"Precedent\",\n",
        "    \"PrecedentOverruled\": \"Precedent\",\n",
        "    \"RatioOfTheDecision\": \"Ratio\",\n",
        "    \"RulingByLowerCourt\": \"RulingL\",\n",
        "    \"RulingByPresentCourt\": \"RulingP\",\n",
        "    \"Statute\": \"Statute\",\n",
        "    \"Dissent\": \"Dissent\",\n",
        "    \"None\": \"None\"\n",
        "}\n",
        "\n",
        "\n",
        "## loading the data\n",
        "data_tr_CL = json.load(f_train_CL)\n",
        "f_train_CL.close()\n",
        "data_te_CL = json.load(f_test_CL)\n",
        "f_test_CL.close()\n",
        "data_tr_IT = json.load(f_train_IT)\n",
        "f_train_IT.close()\n",
        "data_te_IT = json.load(f_test_IT)\n",
        "f_test_IT.close()\n",
        "\n",
        "############ Avoiding none labels ##############\n",
        "def avoid_none(df):\n",
        "  dummy_df = {}\n",
        "  for key in df.keys():\n",
        "    dummy_df[key] = {}\n",
        "    dummy_df[key][\"sentences\"] = []\n",
        "    dummy_df[key][\"complete\"] = []\n",
        "    for i, sentence in enumerate(df[key][\"sentences\"]):\n",
        "      if(df[key][\"complete\"][i] == \"None\"):\n",
        "        #print(\"Found None\")\n",
        "        continue\n",
        "      dummy_df[key][\"sentences\"].append(sentence)\n",
        "      dummy_df[key][\"complete\"].append(label_mapper[df[key][\"complete\"][i]])\n",
        "\n",
        "  return dummy_df\n",
        "\n",
        "### Comment the below line to have None label #####\n",
        "data_tr_IT = avoid_none(data_tr_IT)\n",
        "### Comment the below line to have None label #####\n",
        "data_te_IT = avoid_none(data_te_IT)\n",
        "### Comment the below line to have None label #####\n",
        "data_tr_CL = avoid_none(data_tr_CL)\n",
        "### Comment the below line to have None label #####\n",
        "data_te_CL = avoid_none(data_te_CL)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmU2vLpxsdj5"
      },
      "source": [
        "#### Data conversion #######\n",
        "\n",
        "def json_to_df(data):\n",
        "  sentences_1 = []\n",
        "  sentences_2 = []\n",
        "  label = []\n",
        "  for doc in data.keys():\n",
        "    length_sentences = len(data[doc][\"sentences\"])\n",
        "    for i,sentence in enumerate(data[doc][\"sentences\"]):\n",
        "      if(i== length_sentences-1):\n",
        "        break\n",
        "      sentences_1.append(data[doc][\"sentences\"][i])\n",
        "      sentences_2.append(data[doc][\"sentences\"][i+1])\n",
        "      label_1 = data[doc][\"complete\"][i]\n",
        "      label_2 = data[doc][\"complete\"][i+1]\n",
        "      if label_1 != label_2:\n",
        "        label.append(1)\n",
        "      else:\n",
        "        label.append(0)\n",
        "\n",
        "  df = pd.DataFrame(list(zip(sentences_1, sentences_2, label)), columns =['Sentence 1', 'Sentence 2', \"label\"])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rTq9OMMse0Q"
      },
      "source": [
        "## Converting out data from json to dataframe\n",
        "\n",
        "train_df_CL = json_to_df(data_tr_CL)\n",
        "test_df_CL = json_to_df(data_te_CL)\n",
        "train_df_IT = json_to_df(data_tr_IT)\n",
        "test_df_IT = json_to_df(data_te_IT)\n",
        "\n",
        "comb_df = pd.concat([train_df_IT, train_df_CL])\n",
        "comb_test_df = pd.concat([test_df_IT, test_df_CL])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54hS9x9SsrWo"
      },
      "source": [
        "## installing transformers\n",
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kXOOsd_7s4kF"
      },
      "source": [
        "## importing relevant functions from transformers library that will be used\n",
        "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, BertConfig\n",
        "from transformers import RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig\n",
        "from transformers import XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig\n",
        "from transformers import XLMForSequenceClassification, XLMTokenizer, XLMConfig\n",
        "from transformers import DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig\n",
        "\n",
        "MODEL_CLASSES = {\n",
        "    'bert': (BertForSequenceClassification, BertTokenizer, BertConfig),\n",
        "    'xlnet': (XLNetForSequenceClassification, XLNetTokenizer, XLNetConfig),\n",
        "    'xlm': (XLMForSequenceClassification, XLMTokenizer, XLMConfig),\n",
        "    'roberta': (RobertaForSequenceClassification, RobertaTokenizer, RobertaConfig),\n",
        "    'distilbert': (DistilBertForSequenceClassification, DistilBertTokenizer, DistilBertConfig)}\n",
        "\n",
        "model_type = 'bert' ###--> CHANGE WHAT MODEL YOU WANT HERE!!! <--###\n",
        "model_class, tokenizer_class, config_class = MODEL_CLASSES[model_type]\n",
        "model_name = 'bert-base-uncased'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBCg7iCbs-E1"
      },
      "source": [
        "## loading our tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udVClp1XtKbs"
      },
      "source": [
        "## some more imports\n",
        "import progressbar\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NNcJ4OKtENu"
      },
      "source": [
        "'''\n",
        "    Function to get imput ids for each sentences using the tokenizer\n",
        "'''\n",
        "def input_id_maker(dataf, tokenizer):\n",
        "  input_ids = []\n",
        "  lengths = []\n",
        "  token_type_ids = []\n",
        "\n",
        "  for i in progressbar.progressbar(range(len(dataf['Sentence 1']))):\n",
        "    sen1 = dataf['Sentence 1'].iloc[i]\n",
        "    sen1_t = tokenizer.tokenize(sen1)\n",
        "    sen2 = dataf['Sentence 2'].iloc[i]\n",
        "    sen2_t = tokenizer.tokenize(sen2)\n",
        "    if(len(sen1_t) > 253):\n",
        "      sen1_t = sen1_t[:253]\n",
        "    if(len(sen2_t) > 253):\n",
        "      sen2_t = sen2_t[:253]\n",
        "    CLS = tokenizer.cls_token\n",
        "    SEP = tokenizer.sep_token\n",
        "\n",
        "    sen_full = [CLS] + sen1_t + [SEP] + sen2_t + [SEP]\n",
        "    tok_type_ids_0 = [0 for i in range(len(sen1_t)+2)]\n",
        "    tok_type_ids_1 = [1 for i in range(512-len(sen1_t)-2)]\n",
        "    tok_type_ids = tok_type_ids_0 + tok_type_ids_1\n",
        "    token_type_ids.append(tok_type_ids)\n",
        "    encoded_sent = tokenizer.convert_tokens_to_ids(sen_full)\n",
        "    input_ids.append(encoded_sent)\n",
        "    lengths.append(len(encoded_sent))\n",
        "\n",
        "  input_ids = pad_sequences(input_ids, maxlen=256, value=0, dtype=\"long\", truncating=\"pre\", padding=\"post\")\n",
        "\n",
        "  tok_type_ids = []\n",
        "\n",
        "  return input_ids, lengths, token_type_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNrrMLO5tzyu"
      },
      "source": [
        "## Getting input ids for train and validation set\n",
        "train_input_ids, train_lengths, train_token_type_ids = input_id_maker(train_df_CL, tokenizer)\n",
        "validation_input_ids, validation_lengths, validation_token_type_ids = input_id_maker(test_df_CL, tokenizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvfG4L8nt-9X"
      },
      "source": [
        "'''\n",
        "    This functions returns the attention mask for given input id\n",
        "'''\n",
        "def att_masking(input_ids):\n",
        "  attention_masks = []\n",
        "  for sent in input_ids:\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    attention_masks.append(att_mask)\n",
        "  return attention_masks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOVk8CesuKy1"
      },
      "source": [
        "## getting attention masks and labels for train and val sentences\n",
        "train_attention_masks = att_masking(train_input_ids)\n",
        "validation_attention_masks = att_masking(validation_input_ids)\n",
        "\n",
        "train_labels = train_df_CL['label'].to_numpy().astype('int')\n",
        "validation_labels = test_df_CL['label'].to_numpy().astype('int')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpkVbzSquWnC"
      },
      "source": [
        "## Imports\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from google.colab import drive\n",
        "import textwrap\n",
        "import progressbar\n",
        "import keras\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "train_inputs = train_input_ids\n",
        "validation_inputs = validation_input_ids\n",
        "train_masks = train_attention_masks\n",
        "validation_masks = validation_attention_masks\n",
        "train_tti = train_token_type_ids\n",
        "validation_tti = validation_token_type_ids\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "train_tti = torch.tensor(train_tti)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_masks)\n",
        "validation_tti = torch.tensor(validation_tti)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fpUyR2puaOu"
      },
      "source": [
        "## loading pretrained model\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y25w-qIu2Rq"
      },
      "source": [
        "# max batch size should be 6 due to colab limits\n",
        "batch_size = 6\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_tti, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size = batch_size)\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_tti, validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdmpXoQGujTx"
      },
      "source": [
        "import numpy as np\n",
        "lr = 2e-5\n",
        "max_grad_norm = 1.0\n",
        "epochs = 5\n",
        "num_total_steps = len(train_dataloader)*epochs\n",
        "num_warmup_steps = 1000\n",
        "warmup_proportion = float(num_warmup_steps) / float(num_total_steps)  # 0.1\n",
        "optimizer = AdamW(model.parameters(), lr=lr, correct_bias=True)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_total_steps)\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "seed_val = 2212\n",
        "\n",
        "\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pqWsTZqLuxxF"
      },
      "source": [
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    t0 = time.time()\n",
        "    total_loss = 0\n",
        "\n",
        "    model.train()\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            print('  Batch {:>5,}  of  {:>5,}. '.format(step, len(train_dataloader)))\n",
        "\n",
        "        \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_token_type_ids = batch[2].to(device)\n",
        "        b_labels = batch[3].to(device)\n",
        "\n",
        "        model.zero_grad()        \n",
        "\n",
        "        outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask, labels=b_labels)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        total_loss += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        \n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    for batch in validation_dataloader:\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():        \n",
        "          outputs = model(b_input_ids, token_type_ids=b_token_type_ids, attention_mask=b_input_mask)\n",
        "    \n",
        "        logits = outputs[0]\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shbdiWmwvCBN"
      },
      "source": [
        "prediction_data = validation_data\n",
        "prediction_sampler = validation_sampler\n",
        "prediction_dataloader = validation_dataloader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kjySbj8KzsTx"
      },
      "source": [
        "prediction_inputs = validation_inputs\n",
        "prediction_masks = validation_masks\n",
        "prediction_labels = validation_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Iyhg8j_zlfC"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for (step, batch) in enumerate(prediction_dataloader):\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=b_token_type_ids, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2blJE3Pzn9c"
      },
      "source": [
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "labels_flat = true_labels.flatten()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tqmpXYHBz0cB"
      },
      "source": [
        "flat_accuracy(predictions,true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xozVqlQJz2lK"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "McTSqHm9z-Px"
      },
      "source": [
        "print(classification_report(labels_flat, pred_flat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PM5h9wpKH1S",
        "outputId": "11951062-4a9c-48c0-e385-91dede1520de"
      },
      "source": [
        "validation_input_ids, validation_lengths, validation_token_type_ids = input_id_maker(test_df_IT, tokenizer)\n",
        "validation_attention_masks = att_masking(validation_input_ids)\n",
        "validation_labels = test_df_IT['label'].to_numpy().astype('int')\n",
        "\n",
        "validation_inputs = torch.tensor(validation_input_ids)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "validation_masks = torch.tensor(validation_attention_masks)\n",
        "validation_tti = torch.tensor(validation_token_type_ids)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_tti, validation_labels)\n",
        "validation_sampler = RandomSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size = batch_size)\n",
        "\n",
        "prediction_inputs = validation_inputs\n",
        "prediction_masks = validation_masks\n",
        "prediction_labels = validation_labels\n",
        "\n",
        "prediction_data = validation_data\n",
        "prediction_sampler = validation_sampler\n",
        "prediction_dataloader = validation_dataloader"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100% (1540 of 1540) |####################| Elapsed Time: 0:00:02 Time:  0:00:02\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pVSMz_3KIFG"
      },
      "source": [
        "print('Predicting labels for {:,} test sentences...'.format(len(prediction_inputs)))\n",
        "model.eval()\n",
        "\n",
        "predictions , true_labels = [], []\n",
        "\n",
        "for (step, batch) in enumerate(prediction_dataloader):\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  b_input_ids, b_input_mask, b_token_type_ids, b_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      outputs = model(b_input_ids, token_type_ids=b_token_type_ids, \n",
        "                      attention_mask=b_input_mask)\n",
        "\n",
        "  logits = outputs[0]\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EfFPEizKINH"
      },
      "source": [
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)\n",
        "pred_flat = np.argmax(predictions, axis=1).flatten()\n",
        "labels_flat = true_labels.flatten()\n",
        "flat_accuracy(predictions,true_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GDpEpo1kKIV7"
      },
      "source": [
        "print(classification_report(labels_flat, pred_flat))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SWqhqdTd12AH"
      },
      "source": [
        "## Saving trained model\n",
        "import os\n",
        "\n",
        "output_dir = \"/content/Drive/MyDrive/Technical/RR/SiameseBERT_7labels_full/\" # path to which fine tuned model is to be saved\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ox11twEO2Bvh"
      },
      "source": [
        "## Loading the saved model\n",
        "model = BertForSequenceClassification.from_pretrained(output_dir, output_hidden_states=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkvBFqvX2L1a"
      },
      "source": [
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOCgN94c0Nzv"
      },
      "source": [
        "'''\n",
        "    This function returns the [CLS] embedding for a given input_id and attention mask\n",
        "'''\n",
        "def get_output_for_one_vec(input_id, att_mask):\n",
        "  input_ids = torch.tensor(input_id)\n",
        "  att_masks = torch.tensor(att_mask)\n",
        "  input_ids = input_ids.unsqueeze(0)\n",
        "  att_masks = att_masks.unsqueeze(0)\n",
        "  model.eval()\n",
        "  input_ids = input_ids.to(device)\n",
        "  att_masks = att_masks.to(device)\n",
        "  with torch.no_grad():\n",
        "      output = model(input_ids=input_ids, token_type_ids=None, attention_mask=att_masks)\n",
        "\n",
        "  vec = output[\"hidden_states\"][12][0][0]\n",
        "  vec = vec.detach().cpu().numpy()\n",
        "  return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DoQG6nx1PxO"
      },
      "source": [
        "## Getting embeddings for train sentences\n",
        "clsembs_train = []\n",
        "for i, ii in enumerate(train_input_ids):\n",
        "  clsembs_train.append(get_output_for_one_vec(ii, train_attention_masks[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YrdFn26259H"
      },
      "source": [
        "## Getting embeddings for test sentences\n",
        "clsembs_test = []\n",
        "for i, ii in enumerate(validation_input_ids):\n",
        "  clsembs_test.append(get_output_for_one_vec(ii, validation_attention_masks[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zb97_zcf3kCo"
      },
      "source": [
        "i=0 ## Loading the train embeddings \n",
        "for key in data_tr_CL.keys():\n",
        "  limit = len(data_tr_CL[key][\"sentences\"])\n",
        "  sp = clsembs_train[i:i+limit-1]\n",
        "  np.save(\"/content/Drive/My Drive/Technical/RR/Siamese Net/combined/avoidnone_clsembs_CL_train/\" + key[:-4], np.array(sp))\n",
        "  i = i+limit-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qti0hybc3jfT"
      },
      "source": [
        "i=0 ## ## Loading the train embeddings\n",
        "for key in data_te_CL.keys():\n",
        "  limit = len(data_te_CL[key][\"sentences\"])\n",
        "  sp = clsembs_test[i:i+limit-1]\n",
        "  np.save(\"/content/Drive/My Drive/Technical/RR/Siamese Net/combined/avoidnone_clsembs_CL_test/\" + key[:-4], np.array(sp))\n",
        "  i = i+limit-1"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}