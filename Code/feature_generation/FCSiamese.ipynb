{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FCSiamese.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-UNkBvLFAP3",
        "outputId": "b511c468-54c6-451a-a7d1-5514424ea7dc"
      },
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/Drive\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/Drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8IrHhp_YFg2U"
      },
      "source": [
        "f_train_CL = open(\"/content/Drive/MyDrive/Technical/RR/Data/CL_train.json\", \"r\")\n",
        "f_test_CL = open(\"/content/Drive/MyDrive/Technical/RR/Data/CL_test.json\", \"r\")\n",
        "f_test_IT = open(\"/content/Drive/MyDrive/Technical/RR/Data/IT_test.json\", \"r\")\n",
        "f_train_IT = open(\"/content/Drive/MyDrive/Technical/RR/Data/IT_train.json\", \"r\")\n",
        "\n",
        "data_tr_CL = json.load(f_train_CL)\n",
        "f_train_CL.close()\n",
        "data_te_CL = json.load(f_test_CL)\n",
        "f_test_CL.close()\n",
        "data_tr_IT = json.load(f_train_IT)\n",
        "f_train_IT.close()\n",
        "data_te_IT = json.load(f_test_IT)\n",
        "f_test_IT.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OAUNs3Of6VY"
      },
      "source": [
        "############ Avoiding none labels ##############\n",
        "def avoid_none(df):\n",
        "  dummy_df = {}\n",
        "  for key in df.keys():\n",
        "    dummy_df[key] = {}\n",
        "    dummy_df[key][\"sentences\"] = []\n",
        "    dummy_df[key][\"complete\"] = []\n",
        "    for i, sentence in enumerate(df[key][\"sentences\"]):\n",
        "      if(df[key][\"complete\"][i] == \"None\"):\n",
        "        #print(\"Found None\")\n",
        "        continue\n",
        "      dummy_df[key][\"sentences\"].append(sentence)\n",
        "      dummy_df[key][\"complete\"].append(df[key][\"complete\"][i])\n",
        "\n",
        "  return dummy_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6fCA5P_FtdW"
      },
      "source": [
        "#### Data conversion #######\n",
        "\n",
        "def json_to_df(data, avoid=False):\n",
        "  if(avoid == True):\n",
        "    data = avoid_none(data)\n",
        "  sentences_1 = []\n",
        "  sentences_2 = []\n",
        "  label = []\n",
        "  for doc in data.keys():\n",
        "    length_sentences = len(data[doc][\"sentences\"])\n",
        "    print(length_sentences)\n",
        "    for i,sentence in enumerate(data[doc][\"sentences\"]):\n",
        "      if(i== length_sentences-1):\n",
        "        break\n",
        "      sentences_1.append(data[doc][\"sentences\"][i])\n",
        "      sentences_2.append(data[doc][\"sentences\"][i+1])\n",
        "      label_1 = data[doc][\"complete\"][i]\n",
        "      label_2 = data[doc][\"complete\"][i+1]\n",
        "      if label_1 != label_2:\n",
        "        label.append(1)\n",
        "      else:\n",
        "        label.append(0)\n",
        "\n",
        "  df = pd.DataFrame(list(zip(sentences_1, sentences_2, label)), columns =['Sentence 1', 'Sentence 2', \"label\"])\n",
        "  return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqmfmTOhHcuv"
      },
      "source": [
        "train_df_CL = json_to_df(data_tr_CL, avoid=True)\n",
        "test_df_CL = json_to_df(data_te_CL, avoid=True)\n",
        "train_df_IT = json_to_df(data_tr_IT, avoid=True)\n",
        "test_df_IT = json_to_df(data_te_IT, avoid=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeHybN58jTZI"
      },
      "source": [
        "train_comb_df = pd.concat([train_df_IT, train_df_CL])\n",
        "test_comb_df = pd.concat([test_df_CL, test_df_IT])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yyR_4GIhs21"
      },
      "source": [
        "test_df_IT[\"label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWVH8H53HtV7"
      },
      "source": [
        "!pip install -U sentence-transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jH64EokaHyZS"
      },
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "SB_model = SentenceTransformer('bert-base-nli-max-tokens')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeLDivd7ILc6"
      },
      "source": [
        "model = SB_model\n",
        "sentence_embeddings_1 = model.encode(train_df_IT[\"Sentence 1\"].to_list())\n",
        "sentence_embeddings_2 = model.encode(train_df_IT[\"Sentence 2\"].to_list())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w94_6SbtIbmK"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A2yMQ6AaIxtw"
      },
      "source": [
        "class Sequences(Dataset):\n",
        "    def __init__(self, df, SB_model):\n",
        "        self.labels = df.label.tolist()\n",
        "        self.sentence_1_embeddings = SB_model.encode(df[\"Sentence 1\"].to_list())\n",
        "        self.sentence_2_embeddings = SB_model.encode(df[\"Sentence 2\"].to_list())\n",
        "        self.sequences = []\n",
        "        for i, s1_e in enumerate(self.sentence_1_embeddings):\n",
        "          sentence_diff_embedding = np.absolute(np.array(self.sentence_2_embeddings[i]) - np.array(self.sentence_1_embeddings[i]))\n",
        "          concat_extra = np.concatenate((self.sentence_1_embeddings[i], self.sentence_2_embeddings[i]), axis=0)\n",
        "          concat_full = np.concatenate((concat_extra, sentence_diff_embedding), axis=0)\n",
        "          np_concat_full = np.array(concat_full)\n",
        "          self.sequences.append(np.expand_dims(np_concat_full, axis=0))\n",
        "\n",
        "        self.sequences = np.array(self.sequences)\n",
        "        print(self.sequences)\n",
        "\n",
        "        \n",
        "    def __getitem__(self, i):\n",
        "        return self.sequences[i], self.labels[i]\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.sequences.shape[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNKJ46KIL6yb"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZXSgBHhMBR0"
      },
      "source": [
        "train_dataset = Sequences(train_comb_df, SB_model)\n",
        "test_dataset = Sequences(test_comb_df, SB_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrKmnxrEMQUh"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=1024)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1024)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K12zXOHjNDRV"
      },
      "source": [
        "train_dataset[5][0].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uge2dIwDMmEs"
      },
      "source": [
        "class SiameseClassifier(nn.Module):\n",
        "    def __init__(self, vec_dim, hidden1, hidden2):\n",
        "        super(SiameseClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(vec_dim, hidden1)\n",
        "        self.fc2 = nn.Linear(hidden1, hidden2)\n",
        "        self.fc3 = nn.Linear(hidden2, 1)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        x = F.relu(self.fc1(inputs.squeeze(1).float()))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x, self.fc3(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSkzwZDDUi6r"
      },
      "source": [
        "model = SiameseClassifier(2304, 256, 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LZf3nfNUymz"
      },
      "source": [
        "model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWsILtmzU0t7"
      },
      "source": [
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PhGaYN8mU4Y7"
      },
      "source": [
        "from tqdm import tqdm, tqdm_notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IhIvzr1-U7pO"
      },
      "source": [
        "model.train()\n",
        "train_losses = []\n",
        "for epoch in range(5):\n",
        "    progress_bar = tqdm_notebook(train_loader, leave=False)\n",
        "    losses = []\n",
        "    total = 0\n",
        "    for inputs, target in progress_bar:\n",
        "        model.zero_grad()\n",
        "\n",
        "        output = model(inputs)\n",
        "        loss = criterion(output[1].squeeze(), target.float())\n",
        "        \n",
        "        loss.backward()\n",
        "              \n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
        "\n",
        "        optimizer.step()\n",
        "        \n",
        "        progress_bar.set_description(f'Loss: {loss.item():.3f}')\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        total += 1\n",
        "    \n",
        "    epoch_loss = sum(losses) / total\n",
        "    train_losses.append(epoch_loss)\n",
        "        \n",
        "    tqdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MWXVvtbCVBlo"
      },
      "source": [
        "pred_label_list = []\n",
        "gold_label_list = []\n",
        "model.eval()\n",
        "progress_bar = tqdm_notebook(test_loader, leave=False)\n",
        "for inputs, targets in progress_bar:\n",
        "\n",
        "    outputs = model(inputs)\n",
        "    print(outputs[1].shape)\n",
        "    for output in outputs[1]:\n",
        "      prediction = torch.sigmoid(output).item()\n",
        "      if (prediction > 0.5):\n",
        "        pred_label_list.append(1)\n",
        "      else:\n",
        "        pred_label_list.append(0)\n",
        "\n",
        "    for target in targets:\n",
        "      gold_label_list.append(int(target))\n",
        "    \n",
        "    #epoch_loss = sum(losses) / total\n",
        "    #train_losses.append(epoch_loss)\n",
        "        \n",
        "    #qdm.write(f'Epoch #{epoch + 1}\\tTrain Loss: {epoch_loss:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRr8qCfmVdmP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2l3soSBXeFK"
      },
      "source": [
        "print(gold_label_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8fvfbu-7XhTb"
      },
      "source": [
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cywUvaGOYA2K"
      },
      "source": [
        "print(classification_report(gold_label_list, pred_label_list))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w27x7t2EYGCv"
      },
      "source": [
        "############### SAVING EMBEDDINGS ##############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pfqu817Jiw-4"
      },
      "source": [
        "shift_embs_test = []\n",
        "model.eval()\n",
        "progress_bar = tqdm_notebook(test_loader, leave=False)\n",
        "for inputs, targets in progress_bar:\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    for output in outputs[0]:\n",
        "      npo = output.detach().numpy()\n",
        "      shift_embs_test.append(npo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9Kz19mxi4Nc"
      },
      "source": [
        "shift_embs_train = []\n",
        "model.eval()\n",
        "progress_bar = tqdm_notebook(train_loader, leave=False)\n",
        "for inputs, targets in progress_bar:\n",
        "\n",
        "    outputs = model(inputs)\n",
        "\n",
        "    for output in outputs[0]:\n",
        "      npo = output.detach().numpy()\n",
        "      shift_embs_train.append(npo)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6_GCFCBlcyV"
      },
      "source": [
        "print(len(shift_embs_train))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWCd3XArlfol"
      },
      "source": [
        "i=0\n",
        "\n",
        "### Comment the below line to have None label #####\n",
        "data_tr_IT = avoid_none(data_tr_IT)\n",
        "for key in data_tr_IT.keys():\n",
        "  limit = len(data_tr_IT[key][\"sentences\"])\n",
        "  sp = shift_embs_train[i:i+limit-1]\n",
        "  np.save(\"/content/Drive/My Drive/Technical/RR/Siamese Net/avoidnone_shiftembs_train/\" + key[:-4], np.array(sp))\n",
        "  i = i+limit-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5a0MVLzoZk7"
      },
      "source": [
        "i=0\n",
        "\n",
        "### Comment the below line to have None label #####\n",
        "data_te_IT = avoid_none(data_te_IT)\n",
        "for key in data_te_IT.keys():\n",
        "  limit = len(data_te_IT[key][\"sentences\"])\n",
        "  sp = shift_embs_test[i:i+limit-1]\n",
        "  np.save(\"/content/Drive/My Drive/Technical/RR/Siamese Net/avoidnone_shiftembs_test/\" + key[:-4], np.array(sp))\n",
        "  i = i+limit-1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxVKm2kpoas4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNYTFhyip3SF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}